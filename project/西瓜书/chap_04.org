#+TITLE: 决策树

* 基本流程

#+BEGIN_QUOTE
一般地，一颗决策树包含一个根节点，若干个内部节点和若干个叶节点；叶节点对应于决策
结果，其他每个节点则对应于一个属性测试；每个节点包含的样本集合根据属性测试的结果
被划分到子节点中；根节点包含样本全集，从根节点到每个叶节点的路径对应了一个判定测
试序列。 *决策树学习的目的是为了产生一颗泛化能力强，即处理未见示例能力强的决策树，
其基本流程遵循简单的分而治之策略。*
#+END_QUOTE

- 决策树学习基本算法

#+BEGIN_QUOTE
输入：训练集 $D={(\symbolbold{x}_1, y_1), (\symbolbold{x}_2, y_2), \ldots,
(\symbolbold{x}_m, y_m}$; 属性集 $A={a_1, a_2, \ldots, a_d}$.
过程：函数 TreeGenerate(D, A)
1. 生成结点 node;
2. if $D$ 中样本全属于同一类别 $C$ then
       将 node 标记为 $C$ 类叶结点;
       return
   endif
3. if $A=\emptyset$ OR $D$ 中样本在 $A$ 上取值相同 then
       将 node 标记为叶结点，其类别标记为 $D$ 中样本数最多的类;
       return
   endif
4. 从 $A$ 中选择最优划分属性 $a_\ast$;
5. for $a_\ast$ 的每一个值 $a_\ast^v$ do
       为 node 生成一个分支; 令 $D_v$ 表示 $D$ 中在 $a_\ast$ 上取值为
   $a_{\ast}^v$ 的样本子集;
       if $D_v$ 为空 then
           将分支节点标记为叶结点，其类别标记为 $D$ 中样本最多的类;
           return
       else
           以 TreeGenerate($D_v$, $A \\ \{a_\ast\}$) 为分支结点
       endif
    endfor
输出：以 node 为根结点的一颗决策树
#+END_QUOTE

- 决策树的生成是一个递归过程，在决策树算法中，有三种情形会导致递归返回：
  1. 当前结点包含的样本全属于同一类别，无需划分
  2. 当前属性集为空，或者所有样本在所有属性上的取值相同，无法区分
  3. 当前结点包含的样本集合为空，不能划分

* 划分选择

#+BEGIN_QUOTE
决策学习的关键是如何选择最优划分属性，一般而言，随着划分进程不断进行，我们希望决
策树的分支结点所包含的样本尽可能地属于同一类别，即结点的 “纯度” 越来越高。
#+END_QUOTE

** 信息增益

1. 信息熵 (~information entropy~)

   - 是度量样本集合纯度最常用的一种指标
   - 假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为
     $p_k(k=1,2,\ldots,|\mathcal{Y}|)$, 则 $D$ 的信息熵定义为：
     \begin{equation}
        Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|}p_k\log_2p_k
     \end{equation}
   - Ent(D) 的值越小，则 $D$ 的纯度越高
2. 信息增益 (~information gain~)

   - 假定离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, \ldots, a^V\}$, 若使用
     $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点
     包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$.
   - 考虑到不同分支结点所包含的样本数不同，给分支结点赋予权重 $|D^v|/|D|$, 即样
     本数越多的分支结点影响越大，可以定义用属性 $a$ 对样本集 $D$ 进行划分所获得
     的 “信息增益” (~information gain~)
     \begin{equation}
        Gain(D, a) = Ent(D) - \sum_{v=1}^{V}\frac{D^v}{D}Ent(D^v)
     \end{equation}
   - 一般而言，信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的 “纯度提升”
     越大。因此， *我们可以用信息增益来进行决策树的划分属性选择。*
   - ID3 决策树学习算法就是以信息增益为准则来选择划分属性

** 增益率

#+BEGIN_QUOTE
信息增益准则对可取值数目较多的属性有所偏好，为了减少这种偏好带来的不利影响，著名
的 $C4.5$ 决策树算法不直接使用信息增益，而是使用增益率 (~gain ratio~) 来选择最优
划分属性，增益率定义为
\begin{equation}
    Gain_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}
\end{equation}
其中，
\begin{equation}
    IV(a) = -\sum_{v=1}^V \frac{D^v}{D}\log_2\frac{D^v}{D}
\end{equation}
称为属性 $a$ 的 “固有值” (~intrinstic value~). 属性 $a$ 的可能值数目越多 (V 越大),
则固有值 (IV(a) 越大).
#+END_QUOTE

*增益率准则对可取值数目较少的属性有所偏好，因此，C4.5 算法不是直接选择增益率最大
 的候选划分属性，而是使用了一个启发式*: 先从候选划分属性找出信息增益高出平均水平
 的属性，再从中选择增益率最高的。

** 基尼指数

- 基尼值
  \begin{aligned}
    Gini(D) &= \sum\limits_{k=1}^{|\mathcal{Y}|}\sum\limits_{k^{\prime}\neqk}p_kp_k^{\prime}\\
            &= 1 - \sum\limits_{k=1}^{|\mathcal{Y}|}{p_k}^2
  \end{aligned}
- 直观而言，基尼值反映从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率，
  因此，基尼值越小，数据集 $D$ 纯度越高
- 属性 $a$ 的基尼指数定义为：
  \begin{equation}
    Gini\_index(D, a) = \sum\limits_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)
  \end{equation}
- 在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，
  即 $a_{\ast} = \arg\min\limits_{a\in A}Gini\_index(D, a)$

* 剪枝处理

#+BEGIN_QUOTE
剪枝 (~prunning~) 是决策学习算法对付 “过拟合” 的主要手段。决策树剪枝的基本策略有
“预剪枝” (~preprunning~) 和 “后剪枝” (~postprunning~)。 *预剪枝* 是指在决策树生成过
程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，
则停止划分，并将当前结点标记为叶结点； *后剪枝* 先从训练集生成一个完整的决策树，
然后自底向上对非叶结点进行考察，如果将该结点对应的子树替换为叶结点能带来泛化性能
提升，则将该子树替换为叶结点。为了判断决策树泛化性能是否提升，可以使用留出法，预
留一部分数据作为 “验证集”。
#+END_QUOTE

** 预剪枝
** 后剪枝

#+BEGIN_QUOTE
后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合
风险很小，泛化能力往往优于预剪枝决策树，但后剪枝过程是在生成完全决策树之后进行，
并且要自底向上对树中所有非叶结点进行逐一考察，其训练时间开销比未剪枝决策树和预剪
枝决策树都要大得多。
#+END_QUOTE

* 连续与缺失值

** 连续值

#+BEGIN_QUOTE
对于连续值，最简单的策略是采用二分法，这正是 C4.5 决策树中采取的机制

给定样本集 $D$ 和连续属性 $a$, 假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值，1. 对
这些值从小到大进行排序，记为 $\{a^1, a^2, \ldots, a^n\}$, 2. 基于划分点 $t$ 将
$D$ 划分为子集 $D_t^-$ 和 $D_t^+$, 其中 $D_t^-$ 包含在属性 $a$ 上取值不大于 $t$
的样本， $D_t^+$ 包含在属性 $a$ 上取值大于 $t$ 的样本，3. 对于连续属性 $a$, 可考
察包含 $n-1$ 个元素的候选划分点集合 $T_a = \left\{\frac{a^i+a^{i+1}}{2}|1 \leq i
\leq n-1 \right\}$.

然后，我们就可以像离散属性值一样来考察这些划分点，选取最优划分点进行样本集的划分，
譬如，基尼值可以改写为：
\begin{aligned}
    Gain(D, a) &= \max\limits_{t\in T_a} Gain(D, a, t)\\
               &= \max\limits_{t\in T_a} Ent(D) - \sum\limits_{\lambda\in \{-, +\}}\frac{|D_t^\lambda|}{|D|}}Ent(D_t^\lambda)
\end{aligned}

*与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属
 性。*
#+END_QUOTE

** 缺失值

*** 属性值缺失处理

#+BEGIN_QUOTE
给定训练集 $D$ 和属性 $a$, 令 $\tilde{D}$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样
本子集，假定属性 $a$ 有 $V$ 个可取值 $\{a^1, a^2, \ldots, a^V\}$, 令
$\tilde{D}^v$ 表示 $\tilde{D}$ 在属性 $a$ 上取值为 $a^v$ 的样本子集，
$\tilde{D}_k$ 表示 $\tilde{D}$ 中属于第 $k$ 类 $(k=1,2,\ldots,|\mathcal{Y}|)$ 的
样本子集，则有 $\tilde{D} = \cup_{k=1}^{|\mathcal{Y}|}\tilde{D}_k, \tilde{D} =
\cup_{v=1}^V\tilde{D}^v$, 假定我们为每个样本 $\symbolbold{x}$ 赋予一个权重
$w_{\symbolbold{x}}$, 并定义
\begin{aligned}
    \rho &= \frac{\sum\limits_{\symbolbold{x}\in\tilde{D}}w_{\symbolbold{x}}}{\sum\limits_{\symbolbold{x}\in D}w_{\symbolbold{x}}}\\
    \tilde{p}_k &= \frac{\sum\limits_{\symbolbold{x}\in\tilde{D}_k}w_{\symbolbold{x}}}{\sum\limits_{\symbolbold{x}\in\tilde{D}}w_{\symbolbold{x}}}\quad(1\leq k \leq |\mathcal{Y}|)\\
    \tilde{r}_v &= \frac{\sum\limits_{\symbolbold{x}\in\tilde{D}^v}w_{\symbolbold{x}}}{\sum\limits_{\symbolbold{x}\in\tilde{D}}w_{\symbolbold{x}}}\quad(1\leq v \leq V)\\
\end{aligned}
直观而言， $\rho$ 表示无缺样本所占的比例， $\tilde{p}_k$ 表示无缺值样本中第 $k$
类所占比例， $\tilde{r}_v$ 表示无缺样本中在属性 $a$ 取值为 $a^v$ 的样本所占比例，
显然， $\sum\limits_{k=1}^{|\mathcal{Y}|}\tilde{p}_k = 1,\sum\limits_{v=1}^{V}\tilde{r}_v = 1$

基于以上定义，可将信息增益推广为：
\begin{aligned}
    Gain(D, a) &= \rho \times Gain(\tilde{D}, a)\\
               &= \rho \times \left(Ent(\tilde{D}) - \sum_{v=1}^V\tilde{r}_vEnt(\tilde{D}^v)\right)
\end{aligned}
其中，
\begin{equation}
    Ent(\tilde{D}) = - \sum\limits_{k=1}^{|\mathcal{Y}|}\tilde{p}_k\log_2\tilde{p}_k
\end{equation}
#+END_QUOTE

*** 给定划分属性，样本在该属性上值缺失

#+BEGIN_QUOTE
若样本 $\symbolbold{x}$ 在划分属性 $a$ 上的取值已知，则将 $\symbolbold{x}$ 划入
与其取值对应的子结点，且样本权值在子结点中保持为 $w_{\symbolbold{x}}$. 若样本
$\symbolbold{x}$ 在划分属性 $a$ 上的取值未知，则将 $\symbolbold{x}$ 同时划入所有
子结点，且样本权值在于属性值 $a^v$ 对应的子结点中调整为
$\tilde{r}_v\cdotw_{\symbolbold{x}}$; 直观而言，就是让同一个样本以不同概率划分到
不同的子结点中去。
#+END_QUOTE


* 多变量决策树

- 分类边界的每一段都与坐标轴平行，这样的分类边界使得学习结果由较好的可解释性，因
  为每一段划分都直接对应了某个属性取值
- 真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，如果能使用斜的边
  界，就可以大大简化决策树
- “多变量决策树” (~multivariate decision tree~) 就是能实现 “斜划分” 甚至更复杂划
  分的决策树
- 在此类决策树中，非叶节点不再是仅对某个属性，而是对 *属性的线性组合* 进行测试，
  换言之，每个非叶节点是一个形如 $\sum\limits_{i=1}^dw_ia_i=t$ 的线性分类器， 其
  中， $w_i$ 是属性 $a_i$ 的权重， $w_i$ 和 $t$ 是可在该结点所含的样本集和属性集
  学得。
