#+TITLE: 自然语言处理

* 词嵌入 (work2vec)

#+BEGIN_QUOTE
词向量是用来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的
技术也叫做词嵌入 (word embedding).
#+END_QUOTE

** one-hot 向量缺点

1. one-hot 词向量无法准确表达不同词之间的相似度，如我们常常使用余弦相似度。

   #+BEGIN_QUOTE
   对于向量 $\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^d$, 它们的余弦相似度
   是它们之间夹角的余弦值
   \begin{equation}
     \frac{\boldsymbol{x}^{T}\boldsymbol{y}}{\lVert\boldsymbol{x}\rVert\lVert\boldsymbol{y}\rVert} \in [-1, 1]
   \end{equation}
   由于不同词的 one-hot 向量余弦近似度都为 0，多个不同词之间相似度难以通过
   one-hot 向量准确体现出来。
   #+END_QUOTE

** 跳字模型: 假设基于某个词来生成它在文本序列周围的词

   #+BEGIN_QUOTE
   假设文本序列为 "the" "man" "loves" ”his" "son". 以 "loves" 作为中心词，设背景
   窗口大小为 2，如下图所示，跳字模型所关心的是，给定中心词 "loves", 生成与它距
   离不超过 2 个词的背景词 "the" "man" "his" "son" 的条件概率，即
   \begin{equation}
     P("the", "man", "his", "son"|"loves")
   \end{equation}
   假设给定中心词的情况下，背景词生成互相独立，上式可以改写为：
   \begin{equation}
     P("the"|"loves")\cdotP("man"|"loves")\cdotP("his"|"loves")\cdotP("son"|"loves")
   \end{equation}

   [[file:img/skip-gram.svg]]
   #+END_QUOTE

   在跳字模型中，每个词被表示为两个 $d$ 维向量，用来计算条件概率。假设这个词在词
   典中索引为 $i$, 当它为中心词时，向量表示为 $\boldsymbol{v}_i\in\mathbb{R}^d$,
   而为背景词时词向量表示为 $\boldsymbol{u}_i\in\mathbb{R}^d$. 设中心词 $w_c$ 在
   词典中索引为 $c$, 背景词 $w_0$ 在词典中索引为 $o$, 给定中心词生成背景词的条件
   概率可以通过对向量内积做 softmax 运算得到

   \begin{equation}
     P(w_{o}|w_{c})=\frac{\exp(\boldsymbol{u}_{o}^{T}\boldsymbol{v}_{c})}
     {\sum\limits_{i\in\mathscr{v}}\exp(\boldsymbol{u}_{i}^{T}\boldsymbol{v}_{c})}
   \end{equation}

   其中，词典索引集 $\mathsrc{v} = \{0, 1, \ldots, |\mathscr{v}| - 1\}$.

   假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为 $m$ 时，跳字模型
   的似然函数即给定任一中心词生成所有背景词的概率

   \begin{equation}
    \prod\limits_{t=1}^{T}\prod\limits_{-m\leq j \leq m, j\neq 0}P(w^{(t+j)}|w^{(t)})
   \end{equation}

   这里小于 1 和大于 $T$ 的时间步可以忽略。

** 训练跳字模型

跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中，我们通过最大化似然
函数来学习模型参数，即最大似然估计，等价于最小化以下损失函数

\begin{equation}
  -\sum\limits_{t=1}^{T}\sum\limits_{-m\leq j \leq m, j\neq 0}\log P(w^{(t+j)}|w^{(t)})
\end{equation}

如果使用梯度下降，每一次迭代中，我们随机采样一个较短的子序列来计算有关该子序列的
损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和
背景词向量的梯度。

\begin{equation}
  \log P(w_o|w_c) = \boldsymbol{u}_{o}^{T}\boldsymbol{v}_{c} -
  \log\left(\sum\limits_{i\in\mathscr{v}}\exp(\boldsymbol{u}_{i}^{T}\boldsymbol{v}_{c})\right)
\end{equation}
通过微分，可以得到上式中 $\boldsymbol{v}_c$ 的梯度

\begin{aligned}
  \frac{\partial\log P(w_o|w_c)}{\partial \boldsymbol{v}_c} &=
  \boldsymbol{u}_o - \frac{\sum\limits_{j\in\mathscr{v}}\exp(\boldsymbol{u}_j^T\boldsymbol{v}_c)\boldsymbol{u_j}}
  {\sum\limits_{i\in\mathscr{v}}\exp(\boldsymbol{u}_i^T\boldsymbol{v}_c)}\\
  &=\boldsymbol{u}_o - \sum\limits_{j\in\mathscr{v}}\left(\frac{\exp(\boldsymbol{u}_j^T\boldsymbol{v}_c)}
    {\sum\limits_{i\in\mathscr{v}}\exp(\boldsymbol{u}_i^T\boldsymbol{v}_c)}\right)\boldsymbol{u}_j\\
  &=\boldsymbol{u}_o - \sum\limits_{j\in\mathscr{v}}P(w_j|w_c)\boldsymbol{u}_j
\end{aligned}
其计算需要词典中所有词以 $w_c$ 为中心词的条件概率。有关其他词向量的梯度同理可得。

** 连续词袋模型

连续词袋模型类似跳字模型，不同在于，连续词袋模型假设基于文本序列前后的背景词来生
成该中心词。在同样的文本序列 "the" "man" "loves" "his" "son" 里，以 "loves" 为中
心词，且背景窗口为 2 时，连续词袋模型关心的是，给定背景词 "the" "man" "his"
"son" 生成中心词 "loves" 条件概率，也就是

\begin{equation}
  P("loves"|"the", "man", "his", "son")
\end{equation}

[[file:img/cbow.svg]]

因为连续词袋模型背景词有多个，将这些背景词取平均，然后使用和跳字模型一样的方法计
算条件概率。 设 $\boldsymbol{v}_i\in\mathbb{R}^d$ 和
$\boldsymbol{u}_i\in\mathbb{R}^d$ 分别表示词典中索引为 $i$ 的词作为背景词和中心
词向量 (注意符号的含义与跳字模型中相反)，设中心词 $w_c$ 在词典中索引为 $c$, 背景
词 $w_{o1},\ldots,w_{o2m}$ 在词典中索引为 $o_1,\ldots, o_{2m}$, 那么给定背景词的
生成概率为

\begin{equation}
  P(w_{c}|w_{o_{1}},\ldots,w_{o_{2m}}) = \frac{\exp\left(\frac{1}{2m}\boldsymbol{u_{c}^{T}}
    (\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2m}})\right)}
{\sum\limits_{i\in\mathscr{v}}\exp\left(\frac{1}{2m}\boldsymbol{u_{i}^{T}}
  (\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2m}})\right)}
\end{equation}

为了让符号更简单，记 $\boldsymbol{\mathscr{w}}_0 = \{w_{o_1}, \ldots,
w_{o_2m}\}$, 且 $\bar{\boldsymbol{v}_o}=(v_{o_1} + \ldots + v_{o_2m})/(2m)$, 上
式可以简写为
\begin{equation}
  P(w_{c}|\boldsymbol{\mathscr{w}}) = \frac{\exp(\boldsymbol{u}_{c}^{T}\bar{\boldsymbol{v}}_{o})}
  {\sum\limits_{i\in\mathscr{v}}\exp(\boldsymbol{u}_{i}^{T}\bar{\boldsymbol{v}}_{o})}
\end{equation}

给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的词为 $w^(t)$, 背景窗口大小为 $m$,
连续词袋的似然函数是由背景词生成任一中心词的概率

\begin{equation}
  \prod\limits_{t=1}^{T}P(w^{(t)}|w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)})
\end{equation}

** 训练连续词袋模型

1. 连续词袋模型的最大似然估计等价于最小化损失函数

   \begin{equation}
     -\sum\limits_{t=1}^{T}\log P(w^{(t)}|w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)})
   \end{equation}
 
2. 注意到

   \begin{equation}
     \log P(w_{c}|\boldsymbol{\mathscr{w}}_{o}) = \boldsymbol{u}_{c}^{T}\bar{\boldsymbol{v}}_{o}
     - \log\left(\sum\limits_{i\in\mathscr{v}}\exp(\boldsymbol{u}_{i}^{T}\bar{\boldsymbol{v}}_{o})\right)
   \end{equation}

3. 计算偏微分

   \begin{equation}
     \frac{\partial \log P(w_{c}|\boldsymbol{w}_{o})}{\partial \boldsymbol{v}_{o_{i}}} =
     \frac{1}{2m}\left(\boldsymbol{u}_{c} - \sum\limits_{j\in\mathscr{v}}
       \frac{\exp(\boldsymbol{u}_{j}^{T}\bar{\boldsymbol{v}}_{o})\boldsymbol{u}_{j}}
       {\sum\limits_{i\in\mathscr{v}}\exp(\boldsymbol{u}_{i}^{T}\bar{\boldsymbol{v}}_{o})}\right)
     = \frac{1}{2m}\left(\boldsymbol{u}_{c}-\sum\limits_{j\in\mathscr{v}}P(w_{j}|\bar{\boldsymbol{w}_{o}})\boldsymbol{u}_{j}\right)
   \end{equation}
* 近似训练

#+BEGIN_QUOTE
由于 softmax 运算考虑了背景词可能是词典 $\mathscr{v}$ 中的任一词，不论是跳字模型
还是词袋模型，每一步的梯度运算都包含词典大小数目的项的累加，如果词典过大，梯度计
算开销也会很大，为了降低计算的复杂度，可以采用负采样 (negative sampling) 和层序
(hierarchical softmax).
#+END_QUOTE

** 负采样

1. 负采样修改原来的目标函数，给定中心词 $w_c$ 的一个背景窗口，将背景词 $w_o$ 出
   现在该背景窗口看作一个事件，并将该事件的概率计算为:

   \begin{equation}
     P(D=1|w_{c}, w_{o}) = \sigma(\boldsymbol{u}_{o}^{T}\boldsymbol{v}_{c})
   \end{equation}
   
   其中， $\sigma(x) = \frac{1}{1+\exp(-x)}

2. 最大化文本序列中所有该事件的联合概率来训练词向量，考虑长度为 $T$ 的文本序列，
   时间步 $t$ 的词为 $w^{(t)}$ 且背景窗口大小为 $m$

   \begin{equation}
     \prod\limits_{t=1}^{T}\prod\limits_{-m\leq j \leq m, j\neq 0}=P(D=1|w^{(t)}, w^{(t+j)})
   \end{equation}

   以上模型包含的事件仅考虑了正类样本，这导致当所有词向量相等且值为无穷大时，以
   上的联合概率才被最大化为 1.

3. 负采样通过采样并添加负类样本使目标函数更有意义。设背景词 $w_o$ 出现在中心词
   $w_c$ 的一个背景窗口为事件 $P$, 根据分布 $P(w)$ 采样 $K$ 个未出现在该背景窗口
   中的词，即噪声词。设噪声词 $w_k(k=1,\ldots,K)$ 不出现在中心词 $w_c$ 的该背景
   窗口为事件 $N_k$. 假设同时含有正类样本和负类样本的事件 $P$, $N_1,\ldots,N_K$
   互相独立，负采样将以上需要最大化的仅考虑正类样本的联合概率改写为:

   \begin{equation}
     \prod\limits_{t=1}^{T}\prod\limits_{-m\leq j \leq m, j\neq 0}P(w^{(t+j)}|w^{(t)})
   \end{equation}

   其中，条件概率被近似表示为

   \begin{equation}
     P(w^{(t+j)}|w^{(t)}) = P(D=1|w^{(t)}, w^{(t+j)})
     \prod\limits_{k=1, w_{k}\sim P(w)}^TP(D=0|w^{(t)}, w_{k})
   \end{equation}

4. 设文本序列中时间步 $t$ 的词 $w^{(t)}$ 在词典中的索引为 $i_t$, 噪声词 $w_k$ 在
   词典中的索引为 $h_k$, 以上条件概率的对数损失为

   \begin{aligned}
     -\log P(w^{(t+j)}|w^{(t)}) &= -\log P(D=1|w^{(t)}, w^{(t+j)}) -
     \sum\limits_{k=1, w_{k}\sim P(w)}^{K}\log P(D=0|w^{(t)},w_{k})\\
     &= -\log \sigma\left(\boldsymbol{u}_{i_{t+j}}^T\boldsymbol{v}_{i_t}\right)
     - \sum\limits_{k=1, w_k\sim P(w)}^K \log\left(1-\sigma(\boldsymbol{u}_{h_k}^T\boldsymbol{v}_{i_t}) \right)\\
     &= -\log\sigma\left(\boldsymbol{u}_{i_{t+j}}^T\boldsymbol{v}_{i_t}\right) -
     \sum\limits_{k=1, w_k\sim P(w)}^K \log\left(-\sigma(\boldsymbol{u}_{h_k}^T\boldsymbol{v}_{i_t}) \right)
   \end{aligned}

5. 训练中每一步的梯度计算开销不再与词典大小相关，而是与 $K$ 线性相关。当 $K$ 取
   较小的常数时，负采样在每一步的梯度计算开销较小。
  
** 层序 softmax

[[file:img/hi-softmax.svg]]

1. 层序 softmax 是另一种近似训练法，其使用了二叉树这一数据结构，树的每个叶结点代
   表词典 $\mathscr{v}$ 中的每个词。
2. 假设 $L(w)$ 为从二叉树的根结点到词 $w$ 的叶结点的路径上的结点数。设 $n(w,j)$
   为该路径上第 $j$ 个结点，并设该结点的背景词向量为 $\boldsymbol{u}_{n(w,j)}$.
3. 层序 softmax 将跳字模型中的条件概率近似表示为
   \begin{equation}
     P(w_{o}|w_{c}) = \prod\limits_{j=1}^{L(w_{o})-1}\sigma\left([\![n(w_{o, j+1}) =
       \text{leftChild}(n(w_{o},j))]\!]\cdot\boldsymbol{u}_{n(w_{o},j)}^{\top}\boldsymbol{v}_{c}\right)
   \end{equation}
   其中， $\text{leftChild}(n)$ 是结点 $n$ 的左子节点，如果判断 $x$ 为真，则
   $[\![x]\!]=1$, 反之, $[\![x]\!] = -1$.
4. 计算 $w_c$ 生成词 $w_3$ 的条件概率

   \begin{equation}
     P(w_{3}|w_{c}) = \sigma\left(\boldsymbol{u}_{n(w_{3},1)}^{\top}\boldsymbol{v}_{c}\right)
     \cdot\sigma\left(-\boldsymbol{u}_{n(w_{3}, 2)}^{\top}\boldsymbol{v}_{c}\right)
     \cdot\sigma\left(\boldsymbol{u}_{n(w_{3, 3})}^{\top}\boldsymbol{v}_{c}\right)
   \end{equation}
   由于 $\sigma(x)+\sigma(-x)=1$, 给定中心词 $w_c$ 生成词典 $\mathcal{V}$ 中任一
   词的条件概率之和为 1 这一条件也将满足：
   \begin{equation}
     \sum\limits_{w\in\mathcal{V}}P(w|w_{c})=1.
   \end{equation}

5. 由于 $L(w_o)-1$ 的数量级为 $\mathcal{O}(\log_2|\mathcal{V}|)$, 当词典
   $\mathcal{V}$ 很大时，层序 softmax 在训练中每一步的梯度计算开销相较未使用近似
   计算时大幅降低。
